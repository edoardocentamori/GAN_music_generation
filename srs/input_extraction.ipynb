{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(os.path.join(pathlib.Path(globals()['_dh'][0]).parent, \"data\"), \"js-fakes-16thSeparated.npz\")\n",
    "jsf = np.load(input_path, allow_pickle=True, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token(input_stream, token, begin):\n",
    "    token_shape = list(input_stream.shape)\n",
    "    token_shape[0] = 1\n",
    "    tens_tokens = torch.ones(token_shape) * token\n",
    "    if begin == True:\n",
    "        return torch.cat((tens_tokens, input_stream), axis = 0)\n",
    "    else:\n",
    "        return torch.cat((input_stream, tens_tokens), axis = 0) \n",
    "\n",
    "#[L, bs, F]\n",
    "\n",
    "def song_transform(song, max_lenght):\n",
    "    song = F.pad(song, 'constant', 0)\n",
    "    song = add_token(song, 1, begin=True)\n",
    "    song = song[:max_lenght]\n",
    "    song = add_token(song, -1, begin=False)\n",
    "    return song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create iterator that extract batch of variable size from jsf\n",
    "max_song_length = np.asarray([len(song) for song in jsf['pitches']]).max()\n",
    "\n",
    "class SongIterator():\n",
    "    def __init__(self, song_list):\n",
    "        self.internal_song_list = song_list\n",
    "\n",
    "    def __iter__(self):\n",
    "        #Capire cosa deve tornare\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        x = 1 #Capire cosa deve tornare\n",
    "        return x \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_heads_1, num_heads_2, bs, emb_notes, emb_f):\n",
    "        super(TransformerBlock, self).__init__() # Seq, batch, features\n",
    "        self.ln1 = nn.LayerNorm([bs * num_heads_2 * emb_notes, num_heads_1 * emb_f]) #layer norm: [L, bs, Emb_f, Emb_notes] -> [L, bs, Emb_f, Emb_notes]\n",
    "        self.mha_f = nn.MultiheadAttention(num_heads_1 * emb_f, num_heads_1, dropout=0.25) # multi-head attention per features: [L, BS * Emb_notes, Emb_f] -> [L, BS * Emb_notes, Emb_f]\n",
    "        self.ln2 = nn.LayerNorm([bs * num_heads_1 * emb_f, num_heads_2 * emb_notes]) #layer norm: [BS, L, Emb_f, Emb_notes] -> [BS, L, Emb_f, Emb_notes]\n",
    "        self.mha_l = nn.MultiheadAttention(num_heads_2 * emb_notes, num_heads_2, dropout=0.25) # multi-head attention per lunghezza: [BS * Emb_f, L, Emb_notes] -> [BS * Emb_f, L, Emb_notes]\n",
    "        self.ln3 = nn.LayerNorm([bs, num_heads_1 * emb_f, num_heads_2 * emb_notes]) #layer norm\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_heads_2 * emb_notes, num_heads_2 * emb_notes),  # Linear transformation\n",
    "            nn.LayerNorm([bs, num_heads_1 * emb_f, num_heads_2 * emb_notes]),  # Layer normalization\n",
    "            nn.ELU(),  # Activation function (ELU)\n",
    "            nn.Linear(num_heads_2 * emb_notes, num_heads_2 * emb_notes)  # Linear transformation\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x): # add various reshape\n",
    "        #[L, bs, Emb_f, Emb_notes]\n",
    "        print(\"Step 1, x shape = {}\".format(x.shape))\n",
    "        x_1 = x.transpose(2, 3).reshape((x.shape[0], x.shape[1] * x.shape[3], x.shape[2])) \n",
    "        #[L, BS * Emb_notes, Emb_f]\n",
    "        print(\"Step 2, x_1 shape = {}\".format(x_1.shape))\n",
    "        norm_x_1 = self.ln1(x_1) \n",
    "        print(\"Step 3, x_1 shape = {}\".format(x_1.shape))\n",
    "        attn_output_1 = self.mha_f(norm_x_1, norm_x_1, norm_x_1)[0] # [0] selects the attention output, to be decided if padding is needed\n",
    "        print(\"Step 4, attn_output_1 shape = {}\".format(attn_output_1.shape))\n",
    "        x_2 = x_1 + attn_output_1 # residual connection\n",
    "        print(\"Step 4, x_2 shape = {}\".format(x_2.shape))\n",
    "        x_2 = x_2.reshape(x.shape[0], x.shape[1], x.shape[3], x.shape[2]).transpose(2,3).reshape(x.shape[0], x.shape[1] * x.shape[2], x.shape[3])\n",
    "        #[L, BS * Emb_f, Emb_notes]\n",
    "        print(\"Step 5, x_2 shape = {}\".format(x_2.shape))\n",
    "        norm_x_2 = self.ln2(x_2)\n",
    "        print(\"Step 6, x_norm_x_2 shape = {}\".format(norm_x_2.shape))\n",
    "        attn_output_2 = self.mha_l(norm_x_2, norm_x_2, norm_x_2)[0] # [0] selects the attention output, to be decided if padding is needed\n",
    "        print(\"Step 7, attn_output_2 shape = {}\".format(attn_output_2.shape))\n",
    "        x = x + attn_output_2.reshape(x.shape)\n",
    "        print(\"Step 8, x shape = {}\".format(x.shape))\n",
    "        x = self.ln3(x)\n",
    "        print(\"Step 9, x shape = {}\".format(x.shape))\n",
    "        x = self.mlp(x)\n",
    "        print(\"Step 9, x shape = {}\".format(x.shape))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "emb_notes = 32 \n",
    "emb_f = 4\n",
    "L = 150\n",
    "num_heads_1 = 4\n",
    "num_heads_2 = 4\n",
    "trans_block = TransformerBlock(num_heads_1 = num_heads_1, num_heads_2 = num_heads_2, bs = bs, emb_notes = emb_notes, emb_f = emb_f)\n",
    "#[L, bs, Emb_f, Emb_notes]\n",
    "\n",
    "rand_input = torch.rand((L, bs, num_heads_1 * emb_f, num_heads_2 * emb_notes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_song = torch.randint(36,82,(L, bs, 4))\n",
    "tokens = [0, 1, -1, -2] + list(range(36,82)) # 0: padding, 1: start of signal, -1: end of signal, -2: unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discriminator might relize that padding = real example, find out how to avoid this -> Thus add an attenction mask for padding\n",
    "\n",
    "#Discriminator -> Some Transformer block followed by taking the first token + linear + tanh\n",
    "\n",
    "# Create a tranform operation that remove extra symbol, add \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
