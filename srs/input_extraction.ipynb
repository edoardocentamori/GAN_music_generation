{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(os.path.join(pathlib.Path(globals()['_dh'][0]).parent, \"data\"), \"js-fakes-16thSeparated.npz\")\n",
    "jsf = np.load(input_path, allow_pickle=True, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_heads_1, num_heads_2, bs, emb_notes, emb_f):\n",
    "        super(TransformerBlock, self).__init__() # Seq, batch, features\n",
    "        self.ln1 = nn.LayerNorm([bs * num_heads_2 * emb_notes, num_heads_1 * emb_f]) #layer norm: [L, bs, Emb_f, Emb_notes] -> [L, bs, Emb_f, Emb_notes]\n",
    "        self.mha_f = nn.MultiheadAttention(num_heads_1 * emb_f, num_heads_1, dropout=0.25) # multi-head attention per features: [L, BS * Emb_notes, Emb_f] -> [L, BS * Emb_notes, Emb_f]\n",
    "        self.ln2 = nn.LayerNorm([bs * num_heads_1 * emb_f, num_heads_2 * emb_notes]) #layer norm: [BS, L, Emb_f, Emb_notes] -> [BS, L, Emb_f, Emb_notes]\n",
    "        self.mha_l = nn.MultiheadAttention(num_heads_2 * emb_notes, num_heads_2, dropout=0.25) # multi-head attention per lunghezza: [BS * Emb_f, L, Emb_notes] -> [BS * Emb_f, L, Emb_notes]\n",
    "        self.ln3 = nn.LayerNorm([bs, num_heads_1 * emb_f, num_heads_2 * emb_notes]) #layer norm\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_heads_2 * emb_notes, num_heads_2 * emb_notes),  # Linear transformation\n",
    "            nn.LayerNorm([bs, num_heads_1 * emb_f, num_heads_2 * emb_notes]),  # Layer normalization\n",
    "            nn.ELU(),  # Activation function (ELU)\n",
    "            nn.Linear(num_heads_2 * emb_notes, num_heads_2 * emb_notes)  # Linear transformation\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x): # add various reshape\n",
    "        #[L, bs, Emb_f, Emb_notes]\n",
    "        print(\"Step 1, x shape = {}\".format(x.shape))\n",
    "        x_1 = x.transpose(2, 3).reshape((x.shape[0], x.shape[1] * x.shape[3], x.shape[2])) \n",
    "        #[L, BS * Emb_notes, Emb_f]\n",
    "        print(\"Step 2, x_1 shape = {}\".format(x_1.shape))\n",
    "        norm_x_1 = self.ln1(x_1) \n",
    "        print(\"Step 3, x_1 shape = {}\".format(x_1.shape))\n",
    "        attn_output_1 = self.mha_f(norm_x_1, norm_x_1, norm_x_1)[0] # [0] selects the attention output, to be decided if padding is needed\n",
    "        print(\"Step 4, attn_output_1 shape = {}\".format(attn_output_1.shape))\n",
    "        x_2 = x_1 + attn_output_1 # residual connection\n",
    "        print(\"Step 4, x_2 shape = {}\".format(x_2.shape))\n",
    "        x_2 = x_2.reshape(x.shape[0], x.shape[1], x.shape[3], x.shape[2]).transpose(2,3).reshape(x.shape[0], x.shape[1] * x.shape[2], x.shape[3])\n",
    "        #[L, BS * Emb_f, Emb_notes]\n",
    "        print(\"Step 5, x_2 shape = {}\".format(x_2.shape))\n",
    "        norm_x_2 = self.ln2(x_2)\n",
    "        print(\"Step 6, x_norm_x_2 shape = {}\".format(norm_x_2.shape))\n",
    "        attn_output_2 = self.mha_l(norm_x_2, norm_x_2, norm_x_2)[0] # [0] selects the attention output, to be decided if padding is needed\n",
    "        print(\"Step 7, attn_output_2 shape = {}\".format(attn_output_2.shape))\n",
    "        x = x + attn_output_2.reshape(x.shape)\n",
    "        print(\"Step 8, x shape = {}\".format(x.shape))\n",
    "        x = self.ln3(x)\n",
    "        print(\"Step 9, x shape = {}\".format(x.shape))\n",
    "        x = self.mlp(x)\n",
    "        print(\"Step 9, x shape = {}\".format(x.shape))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "emb_notes = 32 \n",
    "emb_f = 4\n",
    "L = 150\n",
    "num_heads_1 = 4\n",
    "num_heads_2 = 4\n",
    "trans_block = TransformerBlock(num_heads_1 = num_heads_1, num_heads_2 = num_heads_2, bs = bs, emb_notes = emb_notes, emb_f = emb_f)\n",
    "#[L, bs, Emb_f, Emb_notes]\n",
    "\n",
    "rand_input = torch.rand((L, bs, num_heads_1 * emb_f, num_heads_2 * emb_notes))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
