{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token(song, token, begin):\n",
    "    token_shape = list(song.shape)\n",
    "    token_shape[0] = 1\n",
    "    tens_tokens = np.ones(token_shape) * token\n",
    "    if begin == True:\n",
    "        return np.concatenate((tens_tokens, song), axis = 0)\n",
    "    else:\n",
    "        return np.concatenate((song, tens_tokens), axis = 0) \n",
    "    \n",
    "def standardize_song(song, lenght):\n",
    "    if(song.shape[0]>lenght):\n",
    "        return song\n",
    "    song_shape = list(song.shape)\n",
    "    song_shape[0] = lenght - song_shape[0]\n",
    "    song = np.concatenate((song, np.zeros(song_shape)), axis=0)\n",
    "    assert song.shape[0] == lenght\n",
    "    return song\n",
    "\n",
    "def song_transform(song, max_lenght):\n",
    "    song = song[:max_lenght]\n",
    "    song[song == -1] = 82\n",
    "    #song += np.ones_like(song)*np.arange(song.shape[-1])*100\n",
    "    song -= np.ones_like(song) * 34 #Move notes to [36, 82] -> [2, 48]\n",
    "    song = np.pad(song, 0, 'constant')\n",
    "    song = add_token(song, 1, begin=True)\n",
    "    song = add_token(song, 49, begin=False)\n",
    "    song = standardize_song(song, max_lenght+2)\n",
    "    # Final mapping: pad = 0, sos = 1, notes = 2, 47, empy = 48, eos = 49      \n",
    "    return song\n",
    "\n",
    "def batch_uniform(song_batch, max_lenght):\n",
    "    decoded_batch = torch.from_numpy(np.stack([song_transform(song, max_lenght) for song in song_batch], axis=0))\n",
    "    #encoded_batch = F.one_hot(decoded_batch.long(), num_classes=50)\n",
    "    return decoded_batch.transpose(0,1).long()\n",
    "\n",
    "def rand_song_generation(L_distribution, bs, max_lenght):    \n",
    "    songs_batch = []\n",
    "    for _ in range(bs):\n",
    "        L = np.random.choice(L_distribution)\n",
    "        songs_batch.append(song_transform(np.random.randint(36,82,(L, 4)),max_lenght))\n",
    "    decoded_batch = torch.from_numpy(np.stack(songs_batch, axis=0))\n",
    "    #encoded_batch = F.one_hot(decoded_batch.long(), num_classes=50)\n",
    "    return decoded_batch.transpose(0,1).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongIterator():\n",
    "    def __init__(self, song_list, batch_size, song_size, shuffle):\n",
    "        self.step = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.song_size = song_size\n",
    "        if shuffle == True:\n",
    "            self.internal_song_list = random.sample(list(song_list),len(list(song_list)))\n",
    "        else:\n",
    "            self.internal_song_list = list(song_list)\n",
    "                    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        self.step += 1\n",
    "        if self.step > 1000:\n",
    "            raise StopIteration\n",
    "        if (self.step + 1)  * self.batch_size < len(self.internal_song_list):\n",
    "            batch = self.internal_song_list[(self.step * self.batch_size):((self.step + 1) * self.batch_size)]\n",
    "        elif self.step  * self.batch_size < len(self.internal_song_list):\n",
    "            batch = self.internal_song_list[(self.step * self.batch_size):]\n",
    "        else:\n",
    "            raise StopIteration\n",
    "        return batch_uniform(batch, self.song_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract real data\n",
    "\n",
    "input_path = os.path.join(os.path.join(pathlib.Path(globals()['_dh'][0]).parent, \"data\"), \"js-fakes-16thSeparated.npz\")\n",
    "jsf = np.load(input_path, allow_pickle=True, encoding='latin1')\n",
    "len_seq = np.asarray([len(song) for song in jsf['pitches']])\n",
    "\n",
    "max_song_length = len_seq.max()\n",
    "\n",
    "#song_iterator = SongIterator(song_list=jsf['pitches'], batch_size=64, song_size=max_song_length)\n",
    "\n",
    "# test iterator -> Now working\n",
    "# for epoch in range(3):\n",
    "#     for i, batch_song in enumerate(SongIterator(song_list=jsf['pitches'], batch_size=64, song_size=max_song_length, shuffle=True)):\n",
    "#         print(\"Epoch = {}, step = {}, input type = {}, input shape = {}\".format(epoch, i, type(batch_song), batch_song.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing real and fake batch song generation\n",
    "real_batch_song = next(SongIterator(song_list=jsf['pitches'], batch_size=64, song_size=max_song_length, shuffle=True))\n",
    "fake_batch_song = rand_song_generation(len_seq, 64, max_song_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add key_padding_mask\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_heads, bs, emb_notes, emb_f):\n",
    "        super(TransformerBlock, self).__init__() # Seq, batch, features\n",
    "        self.ln1 = nn.LayerNorm([bs * num_heads * emb_notes, emb_f]) #layer norm: [L, bs, Emb_f, Emb_notes] -> [L, bs, Emb_f, Emb_notes]\n",
    "        self.fc1 = nn.Linear(emb_f, emb_f)\n",
    "        self.ln2 = nn.LayerNorm([bs *  emb_f, num_heads * emb_notes]) #layer norm: [BS, L, Emb_f, Emb_notes] -> [BS, L, Emb_f, Emb_notes]\n",
    "        self.mha_l = nn.MultiheadAttention(num_heads * emb_notes, num_heads, dropout=0.25) # multi-head attention per lunghezza: [BS * Emb_f, L, Emb_notes] -> [BS * Emb_f, L, Emb_notes]\n",
    "        self.ln3 = nn.LayerNorm([bs,  emb_f, num_heads * emb_notes]) #layer norm\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_heads * emb_notes, num_heads * emb_notes),  # Linear transformation\n",
    "            nn.LayerNorm([bs, emb_f, num_heads * emb_notes]),  # Layer normalization\n",
    "            nn.ELU(),  # Activation function (ELU)\n",
    "            nn.Linear(num_heads * emb_notes, num_heads * emb_notes)  # Linear transformation\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x): # add various reshape\n",
    "        #[L, bs, Emb_f, Emb_notes]\n",
    "        #print(\"Step TB1, x shape = {}\".format(x.shape))\n",
    "        x_1 = x.transpose(2, 3).reshape((x.shape[0], x.shape[1] * x.shape[3], x.shape[2])) \n",
    "        #[L, BS * Emb_notes, Emb_f]\n",
    "        #print(\"Step TB2, x_1 shape = {}\".format(x_1.shape))\n",
    "        norm_x_1 = self.ln1(x_1) \n",
    "        #print(\"Step TB3, x_1 shape = {}\".format(x_1.shape))\n",
    "        attn_output_1 = self.fc1(norm_x_1)\n",
    "        #print(\"Step TB4, attn_output_1 shape = {}\".format(attn_output_1.shape))\n",
    "        x_2 = x_1 + attn_output_1 # residual connection\n",
    "        #print(\"Step TB5, x_2 shape = {}\".format(x_2.shape))\n",
    "        x_2 = x_2.reshape(x.shape[0], x.shape[1], x.shape[3], x.shape[2]).transpose(2,3).reshape(x.shape[0], x.shape[1] * x.shape[2], x.shape[3])\n",
    "        #[L, BS * Emb_f, Emb_notes]\n",
    "        #print(\"Step TB6, x_2 shape = {}\".format(x_2.shape))\n",
    "        norm_x_2 = self.ln2(x_2)\n",
    "        #print(\"Step TB7, x_norm_x_2 shape = {}\".format(norm_x_2.shape))\n",
    "        attn_output_2 = self.mha_l(norm_x_2, norm_x_2, norm_x_2)[0] # [0] selects the attention output, to be decided if padding is needed\n",
    "        #print(\"Step TB8, attn_output_2 shape = {}\".format(attn_output_2.shape))\n",
    "        x_3 = x + attn_output_2.reshape(x.shape)\n",
    "        #print(\"Step TB9, x shape = {}\".format(x.shape))\n",
    "        x_3 = self.ln3(x_3)\n",
    "        #print(\"Step TB10, x shape = {}\".format(x.shape))\n",
    "        x_3 = self.mlp(x_3)\n",
    "        #print(\"Step TB11, x shape = {}\".format(x.shape))\n",
    "        return x_3 + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(SinusoidalPosEmb, self).__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        position_enc = np.array([[pos / np.power(10000, 2*i/self.dim) for i in range(self.dim)] \n",
    "                                 if pos != 0 else np.zeros(self.dim) for pos in range(x.shape[0])])\n",
    "        # keep dim 0 for padding token position encoding zero vector # To be decided what to do with this\n",
    "        position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # dim 2i\n",
    "        position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # dim 2i+1\n",
    "        position_enc = np.expand_dims(position_enc, axis=(1,2))\n",
    "        position_enc = np.repeat(position_enc, repeats=x.shape[1], axis=1)\n",
    "        position_enc = np.repeat(position_enc, repeats=x.shape[2], axis=2)\n",
    "        return torch.from_numpy(position_enc).type(torch.FloatTensor)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_layers, num_emb, emb_f, num_heads, emb_notes, bs):\n",
    "        super(Generator, self).__init__()\n",
    "        #[L, bs, Emb_f, notes]\n",
    "        self.embedding = nn.Embedding(num_emb, num_heads * emb_notes) # To be refined\n",
    "        self.embedding.weight.data = 0.001 * self.embedding.weight.data # Unclear\n",
    "        #[L, bs, Emb_f, num_heads_1 * emb_f]\n",
    "        self.pos_emb = SinusoidalPosEmb(num_heads * emb_notes)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(num_heads, bs, emb_notes, emb_f) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(num_heads * emb_notes, num_heads * emb_notes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"Gen Step 1, x shape = {}\".format(x.shape))\n",
    "        input_emb = self.embedding(x)\n",
    "        #print(\"Gen Step 2, input_emb shape = {}\".format(input_emb.shape))\n",
    "        pos_emb = self.pos_emb(x)\n",
    "        #print(\"Gen Step 3, pos_emb shape = {}\".format(pos_emb.shape))\n",
    "        emb = input_emb + pos_emb\n",
    "        #print(\"Gen Step 4, emb shape = {}\".format(emb.shape))\n",
    "        for block in self.blocks:\n",
    "            emb = block(emb) # add key_padding_mask\n",
    "            #print(\"Gen Step 5 loop, emb shape = {}\".format(emb.shape))\n",
    "        emb = self.fc_out(emb)\n",
    "        return torch.argmax(emb, dim = -1)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_layers, num_emb, emb_f, num_heads, emb_notes, bs, len_seq):\n",
    "        super(Discriminator, self).__init__()\n",
    "        #[L, bs, Emb_f, notes]\n",
    "        self.embedding = nn.Embedding(num_emb, num_heads * emb_notes) # To be refined\n",
    "        self.embedding.weight.data = 0.001 * self.embedding.weight.data # Unclear\n",
    "        #[L, bs, Emb_f, num_heads_1 * emb_f]\n",
    "        self.pos_emb = SinusoidalPosEmb(num_heads * emb_notes)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(num_heads, bs, emb_notes, emb_f) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(len_seq * emb_f * num_heads * emb_notes, 2) \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"1. shape = {}\".format(x.shape))\n",
    "        input_emb = self.embedding(x)\n",
    "        print(\"2. shape = {}\".format(input_emb.shape))\n",
    "        pos_emb = self.pos_emb(x)\n",
    "        print(\"3. shape = {}\".format(pos_emb.shape))\n",
    "        emb = input_emb + pos_emb\n",
    "        print(\"4. shape = {}\".format(emb.shape))\n",
    "        for block in self.blocks:\n",
    "            emb = block(emb) # add key_padding_mask\n",
    "        print(\"5. Shape = {}\".format(emb.shape))\n",
    "        emb = emb.transpose(0, 1).reshape(emb.shape[1], emb.shape[0] * emb.shape[2] * emb.shape[3])\n",
    "        print(\"6. Shape = {}\".format(emb.shape))\n",
    "        emb = self.fc_out(emb)\n",
    "        return F.softmax(emb, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Losses definition\n",
    "def gan_discriminator_loss(output, real_label=True):\n",
    "    if real_label:\n",
    "        labes = torch.ones_like(output) * np.arange(0,2)\n",
    "    else:\n",
    "        labes = torch.ones_like(output) * np.arange(1,-1,-1)\n",
    "    return F.binary_cross_entropy_with_logits(output, labes)\n",
    "    \n",
    "def gan_generator_loss(output, real_label=True):\n",
    "    if real_label:\n",
    "        return -output.mean()\n",
    "    else:     \n",
    "        return output.mean("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test TransformerBlock -> Work\n",
    "bs = 4\n",
    "emb_notes = 20 \n",
    "emb_f = 4\n",
    "num_heads = 2\n",
    "trans_block = TransformerBlock(num_heads = num_heads, bs = bs, emb_notes = emb_notes, emb_f = emb_f)\n",
    "#[L, bs, Emb_f, Emb_notes]\n",
    "\n",
    "# S soprano (100), A alto (200), T tenor (300), B bass (400). \n",
    "\n",
    "n_tokens = len([0, 1, -1, 82]) + len(range(36,82))\n",
    "batch_song = next(SongIterator(song_list=jsf['pitches'], batch_size=64, song_size=max_song_length, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([522, 4, 4])\n",
      "1. shape = torch.Size([522, 4, 4])\n",
      "2. shape = torch.Size([522, 4, 4, 40])\n",
      "3. shape = torch.Size([522, 4, 4, 40])\n",
      "4. shape = torch.Size([522, 4, 4, 40])\n",
      "5. Shape = torch.Size([522, 4, 4, 40])\n",
      "6. Shape = torch.Size([4, 83520])\n",
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "#[L, bs, Emb_f, Emb_notes]\n",
    "#[bs, L, Emb_f, Emb_notes]]\n",
    "\n",
    "# Test that Generator work as expected \n",
    "num_layers = 3\n",
    "num_emb = 10 # to be adjusted  \n",
    "Gen = Generator(num_layers, n_tokens, emb_f, num_heads, emb_notes, bs)\n",
    "Dis = Discriminator(num_layers, n_tokens, emb_f, num_heads, emb_notes, bs, max_song_length+2)\n",
    "noise_input = rand_song_generation(len_seq, bs, max_song_length)\n",
    "a = Gen(noise_input.type(torch.int32))\n",
    "print(a.shape)\n",
    "b = Dis(a)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(b) * np.arange(1,-1,-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
